{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Guardian Opinions: Text Classification and Bag-of-Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Import and explore data\n",
    "- Apply text preprocessing techniques\n",
    "- -Implement the bag-of-words model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and explore data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data was collected using a Scrapy spider, found in this repository under /guardianscraper/guardianscraper/spiders/guardianspider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>Liz Truss set out her grand plan to me – and i...</td>\n",
       "      <td>Katy Balls</td>\n",
       "      <td>“You’ve set the cat among the pigeons,” messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>The Gareth Thomas case proves it: no one wins ...</td>\n",
       "      <td>James Greig</td>\n",
       "      <td>Few public figures alive today have done more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>There’s no cycle of violence in Jerusalem – on...</td>\n",
       "      <td>Jalal Abukhater</td>\n",
       "      <td>Almost every day, the bulldozers are on the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023/feb/06</td>\n",
       "      <td>The ‘leftwing economic establishment’ did not ...</td>\n",
       "      <td>Polly Toynbee</td>\n",
       "      <td>“This soul-searching has not been easy,” she w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>I have seen race hate in the US and UK and the...</td>\n",
       "      <td>Al Sharpton</td>\n",
       "      <td>I came to London more than 30 years ago to pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>As the detective who inspired TV’s Prime Suspe...</td>\n",
       "      <td>Jackie Malton</td>\n",
       "      <td>We have now heard for the first time from Davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>Britain, we had a thing with Truss and Johnson...</td>\n",
       "      <td>Marina Hyde</td>\n",
       "      <td>Liz Truss is now eluded by two major types of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>Would I ditch a date after 51 minutes? I’ve wa...</td>\n",
       "      <td>Elle Hunt</td>\n",
       "      <td>Fifty-one minutes. It’s too long for a meeting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>As an ex police officer, this much is clear: a...</td>\n",
       "      <td>Steve White</td>\n",
       "      <td>The sentencing of David Carrick and the dreadf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023/feb/07</td>\n",
       "      <td>Jeremy Hunt says focus on the ‘economically in...</td>\n",
       "      <td>Frances Ryan</td>\n",
       "      <td>As the government lurches between screw-ups, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                              title  \\\n",
       "0  2023/feb/07  Liz Truss set out her grand plan to me – and i...   \n",
       "1  2023/feb/07  The Gareth Thomas case proves it: no one wins ...   \n",
       "2  2023/feb/07  There’s no cycle of violence in Jerusalem – on...   \n",
       "3  2023/feb/06  The ‘leftwing economic establishment’ did not ...   \n",
       "4  2023/feb/07  I have seen race hate in the US and UK and the...   \n",
       "5  2023/feb/07  As the detective who inspired TV’s Prime Suspe...   \n",
       "6  2023/feb/07  Britain, we had a thing with Truss and Johnson...   \n",
       "7  2023/feb/07  Would I ditch a date after 51 minutes? I’ve wa...   \n",
       "8  2023/feb/07  As an ex police officer, this much is clear: a...   \n",
       "9  2023/feb/07  Jeremy Hunt says focus on the ‘economically in...   \n",
       "\n",
       "            author                                            article  \n",
       "0       Katy Balls  “You’ve set the cat among the pigeons,” messag...  \n",
       "1      James Greig  Few public figures alive today have done more ...  \n",
       "2  Jalal Abukhater  Almost every day, the bulldozers are on the mo...  \n",
       "3    Polly Toynbee  “This soul-searching has not been easy,” she w...  \n",
       "4      Al Sharpton  I came to London more than 30 years ago to pro...  \n",
       "5    Jackie Malton  We have now heard for the first time from Davi...  \n",
       "6      Marina Hyde  Liz Truss is now eluded by two major types of ...  \n",
       "7        Elle Hunt  Fifty-one minutes. It’s too long for a meeting...  \n",
       "8      Steve White  The sentencing of David Carrick and the dreadf...  \n",
       "9     Frances Ryan  As the government lurches between screw-ups, s...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./guardian_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_article = df.article.values[0]\n",
    "sample_article"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some text clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article'] = df['article'].replace('\\n', '', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Understand the bag of words model\n",
    "- Tokenization\n",
    "- Stop-word removal\n",
    "- Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bag of words intuition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a list of all the words in the articls\n",
    "2. Convert each article into vector counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some limitations:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. Possibly too many words\n",
    "   2. some words may occur too frequently\n",
    "   3. some words may occur very rarely or only once\n",
    "   4. A single word may have many forms (ex: go, gone, going; or bird vs. bird)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/juancarlos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenize = word_tokenize(df.article.values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "\n",
    "for text in df.article.values:\n",
    "    all_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50074\n"
     ]
    }
   ],
   "source": [
    "# join all text\n",
    "all_joined = ' '.join(all_text)\n",
    "full_tokens = word_tokenize(all_joined)\n",
    "print(len(full_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-word removal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing commonly-occurring words that offer little or no meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juancarlos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(tokens):\n",
    "  return[ word for word in tokens if word.lower() not in english_stops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31076\n"
     ]
    }
   ],
   "source": [
    "stopless_tokens = remove_stops(full_tokens)\n",
    "\n",
    "print(len(stopless_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs. Lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While stemming truncates variations of a word to a common 'stem' (sometimes this tem is not grammatical, i.e. 'decidedly' => 'decid'), Lemmatization finds the grammatical root word (i.e. \"love\" => \"love\", \"loving\" => \"love\", \"lovable\" => \"love\" ). Lemmatization finds the root word by using a dictionary, thus making it slow and heavy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbba1214f6436c1b9df455eb4b634c6f3489084cc9221bb041863e864fbbb981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
