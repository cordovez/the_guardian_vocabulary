{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathered Notebook\n",
    "\n",
    "This notebook was generated by the Gather Extension. The intent is that it contains only the code and cells required to produce the same results as the cell originally selected for gathering. Please note that the Python analysis is quite conservative, so if it is unsure whether a line of code is necessary for execution, it will err on the side of including it.\n",
    "\n",
    "**Please let us know if you are satisfied with what was gathered [here](https://aka.ms/gatherfeedback).**\n",
    "\n",
    "Thanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./latestopinions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].replace('\\n', '', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82560\n"
     ]
    }
   ],
   "source": [
    "text_list = df[\"text\"].to_list()\n",
    "\n",
    "all_text = \" \".join(text_list)\n",
    "add_space = re.sub(r'\\b\\.(?=\\w)', \". \", all_text)\n",
    "print(len(add_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alltext.txt', 'w') as f:\n",
    "    f.write(add_space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('popped', 'VBD'),\n",
       " ('into', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('local', 'JJ'),\n",
       " ('newsagent', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('pick', 'VB'),\n",
       " ('up', 'RP'),\n",
       " ('a', 'DT'),\n",
       " ('copy', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Guardian', 'NNP'),\n",
       " ('newspaper', 'NN'),\n",
       " ('and', 'CC'),\n",
       " (',', ','),\n",
       " ('as', 'IN')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = word_tokenize(add_space)\n",
    "pos_text = nltk.pos_tag(tokenized_text)\n",
    "# nltk.help.upenn_tagset('NN.*')\n",
    "print(len(pos_text))\n",
    "pos_text[:20]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juancarlos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops = stopwords.words('english')\n",
    "\", \".join(english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('popped', 'VBD'),\n",
       " ('local', 'JJ'),\n",
       " ('newsagent', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('pick', 'VB'),\n",
       " ('copy', 'NN'),\n",
       " ('Guardian', 'NNP'),\n",
       " ('newspaper', 'NN'),\n",
       " (',', ',')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopless_tuples =[]\n",
    "\n",
    "for tuple in pos_text:\n",
    "    if tuple[0].lower() not in english_stops:\n",
    "        stopless_tuples.append(tuple)\n",
    "print(len(stopless_tuples))\n",
    "stopless_tuples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = {}\n",
    "\n",
    "# for word in all_text:\n",
    "#     if word.lower() not in english_stops:\n",
    "#         counts[word] = counts.get(word, 0) + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View some of the \"counts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_df = pd.DataFrame.from_dict(counts, orient='index', columns=['count'])\n",
    "# peak_df.head(20).sort_values(\"count\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclude punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('popped', 'VBD'),\n",
       " ('local', 'JJ'),\n",
       " ('newsagent', 'NN'),\n",
       " ('last', 'JJ'),\n",
       " ('week', 'NN'),\n",
       " ('pick', 'VB'),\n",
       " ('copy', 'NN'),\n",
       " ('Guardian', 'NNP'),\n",
       " ('newspaper', 'NN'),\n",
       " ('stood', 'VBD'),\n",
       " ('queue', 'NN'),\n",
       " ('realised', 'VBD'),\n",
       " ('standing', 'VBG'),\n",
       " ('next', 'JJ'),\n",
       " ('rack', 'NN'),\n",
       " ('Valentine', 'NNP'),\n",
       " ('Day', 'NNP'),\n",
       " ('cards', 'NNS'),\n",
       " ('began', 'VBD'),\n",
       " ('browse', 'VB')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_vocab = []\n",
    "exclude = [\",\", \".\",\"“\", \"”\", \"‘\", \"’\" ,\"-webkit\", \"-ms\", \"flex\", \"email-\", \"nowrap\", \"width:\", \"height:\"]\n",
    "for tuple in stopless_tuples:\n",
    "     if tuple[0] not in exclude:\n",
    "        clean_vocab.append(tuple)\n",
    "print(len(clean_vocab))\n",
    "clean_vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3974\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "\n",
    "for tuple in clean_vocab:\n",
    "    if tuple in counts:\n",
    "        counts[tuple] += 1\n",
    "    else:\n",
    "        counts[tuple] = 1\n",
    "\n",
    "clean_vocab = [(tuple[0], tuple[1], count) for tuple, count in counts.items()]\n",
    "print(len(clean_vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3919\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for tuple in clean_vocab:\n",
    "    if not tuple[0].isdigit():\n",
    "        vocab.append(tuple)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('popped', 'VBD', 1),\n",
       " ('local', 'JJ', 3),\n",
       " ('newsagent', 'NN', 1),\n",
       " ('last', 'JJ', 18),\n",
       " ('week', 'NN', 12),\n",
       " ('pick', 'VB', 1),\n",
       " ('copy', 'NN', 1),\n",
       " ('Guardian', 'NNP', 1),\n",
       " ('newspaper', 'NN', 1),\n",
       " ('stood', 'VBD', 4),\n",
       " ('queue', 'NN', 2),\n",
       " ('realised', 'VBD', 1),\n",
       " ('standing', 'VBG', 2),\n",
       " ('next', 'JJ', 10),\n",
       " ('rack', 'NN', 1),\n",
       " ('Valentine', 'NNP', 6),\n",
       " ('Day', 'NNP', 6),\n",
       " ('cards', 'NNS', 9),\n",
       " ('began', 'VBD', 2),\n",
       " ('browse', 'VB', 1)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>hateful</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3910</th>\n",
       "      <td>bile</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>start</td>\n",
       "      <td>VBP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>fires</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>fanning</td>\n",
       "      <td>VBG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>flames</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>writes</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>racism</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>liberties</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  POS  Count\n",
       "3909    hateful   JJ      1\n",
       "3910       bile   NN      1\n",
       "3911      start  VBP      1\n",
       "3912      fires  NNS      1\n",
       "3913    fanning  VBG      1\n",
       "3914     flames  NNS      1\n",
       "3915     Taylor  NNP      1\n",
       "3916     writes  VBZ      1\n",
       "3917     racism   NN      1\n",
       "3918  liberties  NNS      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peek = pd.DataFrame(vocab, columns=['Word', 'POS', 'Count'])\n",
    "peek.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>popped</td>\n",
       "      <td>VBD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>JJ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newsagent</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>last</td>\n",
       "      <td>JJ</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>week</td>\n",
       "      <td>NN</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>flames</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3916</th>\n",
       "      <td>writes</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>racism</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>liberties</td>\n",
       "      <td>NNS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3919 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  POS  Count\n",
       "0        popped  VBD      1\n",
       "1         local   JJ      3\n",
       "2     newsagent   NN      1\n",
       "3          last   JJ     18\n",
       "4          week   NN     12\n",
       "...         ...  ...    ...\n",
       "3914     flames  NNS      1\n",
       "3915     Taylor  NNP      1\n",
       "3916     writes  VBZ      1\n",
       "3917     racism   NN      1\n",
       "3918  liberties  NNS      1\n",
       "\n",
       "[3919 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peek"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "1. find if there is a difference between proper nouns and other capitalised words and lower case words that don't need to be capitalised\n",
    "2. export to .csv without index\n",
    "3. add extended name for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('tuples.txt', 'w') as f:\n",
    "#     for t in vocab:\n",
    "#         f.write(str(t) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "exceptions = [t for t in vocab if t[0].startswith(\"-webkit-\")]\n",
    "print(len(exceptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_chars(s):\n",
    "#     return s.translate(str.maketrans(\" \", \" \", string.punctuation.replace(\"_\", \"\")))\n",
    "\n",
    "# clean_count = {replace_chars(k): v for k, v in counts.items()}\n",
    "# print(len(clean_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k.replace(',', ' ').replace('.', ' '): v for k, v in counts.items()}\n",
    "\n",
    "# print(len(clean_count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delete quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k.replace('“', '').replace('”', '').replace('‘', '').replace('’', ''): v for k, v in clean_count.items()}\n",
    "# print(len(clean_count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove entries with capitalized words (potentially proper nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k: v for k, v in clean_count.items() if len(k)>0 and not k[0].isupper()}\n",
    "# print(len(clean_count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove entries with sterling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k: v for k, v in clean_count.items() if len(k)>0 and not k[0]== \"£\"}\n",
    "# # peek_currency = {k: v for k, v in clean_count.items() if k[0]== \"£\"}\n",
    "# print(len(clean_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove entries with \"–\" as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k: v for k, v in clean_count.items() if len(k)>0 and not k[0]== \"–\"}\n",
    "# # peek_currency = {k: v for k, v in clean_count.items() if k[0]== \"£\"}\n",
    "# print(len(clean_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove entries with digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_count = {k: v for k, v in clean_count.items() if len(k)>0 and not k[0].isdigit()}\n",
    "# print(len(clean_count))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek = pd.DataFrame.from_dict(clean_count, orient='index', columns=['count'])\n",
    "# peek.sort_values(\"count\").tail(50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_vocabulary = sorted([ (v,k) for k,v in counts.items()], reverse=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorted list reveals that further refinement is going to be needed: The most common are probably only marginally useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_vocabulary[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many at the bottom of the list (those less used) are not grammatical words and they need to be cleaned up. For example removing words in parenthesis, with hashtags, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_vocabulary[-1000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_counts = {k: v for k, v in counts.items() if not (k.startswith('#') \n",
    "# or k.startswith('(')\n",
    "# or k.startswith('$')\n",
    "# or k.startswith('£')\n",
    "# or k.startswith('\\''))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(sorted_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(clean_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in list(clean_counts)[900:1000]:\n",
    "#     print (\"key {}, value {} \".format(x,  clean_counts[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [(k, v) for k, v in clean_counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame(data, columns=['key', 'value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.sort_values(by='value', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbba1214f6436c1b9df455eb4b634c6f3489084cc9221bb041863e864fbbb981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
